from approx_query_engine import ApproxQueryEngine
from synthetic_data import generate_sales_data     
import time
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def run_benchmark(streaming: bool = False):
    print("Approximate Query Engine Demo")
    print("=" * 70)
    
    # Test data sizes
    sizes = [100_000, 1_000_000]
    results = []
    
    for size in sizes:
        print(f"\nTesting with {size:,} rows")
        print("-" * 70)
        
        # Generate and load data
        df = generate_sales_data(size)
        engine = ApproxQueryEngine()  # Start in streaming mode if requested
        
        if streaming:
            # Simulate streaming by adding data in chunks
            chunk_size = size // 10
            for i in range(0, size, chunk_size):
                engine.add_streaming_data(df.iloc[i:i+chunk_size])
                print(f"Processed {min(i+chunk_size, size):,} records...")
        else:
            engine = ApproxQueryEngine(df)
        
        # Test queries with varying complexity
        test_queries = [
            ("Simple Count", "SELECT COUNT(*) FROM sales;"),
            ("Distinct Count", "SELECT COUNT(DISTINCT user_id) FROM sales;"),
            ("Group By Count", "SELECT category, COUNT(*) FROM sales GROUP BY category;"),
            ("Complex Aggregate", "SELECT category, COUNT(*), AVG(price), SUM(price) FROM sales GROUP BY category;"),
        ]
        
        print("\nRunning benchmarks...")
        print("-" * 70)
        
        for name, query in test_queries:
            print(f"\nTest: {name}")
            print(f"Query: {query}")
            
            # Run exact query as baseline
            exact = engine.run_query(query)
            if 'error' in exact:
                print(f"❌ Error in exact query: {exact['error']}")
                continue
                
            print(f"\nExact Results:")
            print(f"Time: {exact['time']:.4f} seconds")
            print(exact['result'])
            
            # Test different accuracy levels
            for accuracy in [90, 95, 99]:
                approx = engine.run_query(
                    f"APPROXIMATE {query[:-1]} WITH ACCURACY {accuracy}% WITHIN {95}% CONFIDENCE;"
                )
                
                if 'error' in approx:
                    print(f"Error in approximate query: {approx['error']}")
                    continue
                
                speedup = exact['time'] / max(approx['time'], 1e-6)
                
                result = {
                    'size': size,
                    'query': name,
                    'accuracy': accuracy,
                    'exact_time': exact['time'],
                    'approx_time': approx['time'],
                    'speedup': speedup,
                    'confidence_interval': approx['ci']
                }
                
                # Calculate actual error for metrics we can compare
                if 'GROUP BY' not in query:
                    # For non-grouped queries, compare the first numeric column
                    exact_val = exact['result'].select_dtypes(include=['int64', 'float64']).iloc[0,0]
                    approx_val = approx['result'].select_dtypes(include=['int64', 'float64']).iloc[0,0]
                    actual_error = abs(exact_val - approx_val) / exact_val * 100
                    result['actual_error'] = actual_error
                
                results.append(result)
                
                print(f"\nApproximate Results (Target Accuracy: {accuracy}%):")
                print(f"Time: {approx['time']:.4f} seconds ({speedup:.1f}x faster)")
                print(f"Confidence Interval: ±{approx['ci']*100:.1f}%")
                if 'actual_error' in result:
                    print(f"Actual Error: {actual_error:.1f}%")
                print(approx['result'])
                
                # Performance assessment
                if speedup >= 3:
                    print("Exceeds 3x speed target")
                elif speedup > 1:
                    print("Faster but below 3x target")
                else:
                    print("Slower than exact query")
    
    # Convert results to DataFrame for analysis
    results_df = pd.DataFrame(results)
    
    # Plot performance comparison
    plt.figure(figsize=(12, 6))
    sns.boxplot(data=results_df, x='query', y='speedup')
    plt.title('Query Performance Comparison')
    plt.xticks(rotation=45)
    plt.ylabel('Speedup Factor (vs Exact Query)')
    plt.tight_layout()
    plt.savefig('performance_comparison.png')
    
    # Summary statistics
    print("\nPerformance Summary:")
    print("-" * 70)
    summary = results_df.groupby('query').agg({
        'speedup': ['mean', 'min', 'max'],
        'actual_error': ['mean', 'min', 'max']
    }).round(2)
    print("\nSpeedup Factors:")
    print(summary['speedup'])
    print("\nError Rates (where applicable):")
    print(summary['actual_error'])

if __name__ == "__main__":
    # Test both batch and streaming modes
    print("\nBatch Mode Test")
    run_benchmark(streaming=False)
    
    print("\nStreaming Mode Test")
    run_benchmark(streaming=True)
