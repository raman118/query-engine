from typing import Dict, Any, List, Union, Optionalfrom typing import Dict, Any, List, Union, Optionalfrom typing import Dict, Any, List, Union, Optionalfrom typing import Dict, Any, List, Union, Optional, Tuplefrom typing import Dict, Any, List, Union, Optional, Tuple

import pandas as pd

import numpy as npimport pandas as pd

import re

import timeimport numpy as npimport pandas as pd

from sketches import HyperLogLog, CountMinSketch, ReservoirSampler

from collections import defaultdictimport re



class ApproxQueryEngine:import timeimport numpy as npimport pandas as pdimport pandas as pd

    def __init__(self, df: Optional[pd.DataFrame] = None):

        """Initialize the query engine with optional initial DataFrame."""from sketches import HyperLogLog, CountMinSketch, ReservoirSampler

        self.df = df.copy() if df is not None else pd.DataFrame()

        self.streaming_mode = df is Nonefrom collections import defaultdictimport re

        self._cache = {

            'total_rows': len(self.df) if df is not None else 0,

            'category_stats': None,

            'sketches': {},class ApproxQueryEngine:import timeimport numpy as npimport numpy as np

            'samples': {}

        }    def __init__(self, df: Optional[pd.DataFrame] = None):

        self._setup_cache()

        """Initialize the query engine with optional initial DataFrame."""from sketches import HyperLogLog, CountMinSketch, ReservoirSampler

    def _setup_cache(self):

        """Initialize cache with sketches and samples for key columns."""        self.df = df.copy() if df is not None else pd.DataFrame()

        if not self.streaming_mode and len(self.df) > 0:

            # Initialize category-based statistics        self.streaming_mode = df is Nonefrom collections import defaultdictimport reimport re

            self._cache['category_stats'] = (

                self.df.groupby('category', as_index=False)        self._cache = {

                .agg({'price': ['count', 'mean', 'sum']})

            )            'total_rows': len(self.df) if df is not None else 0,

            self._cache['category_stats'].columns = ['category', 'count', 'avg_price', 'sum_price']

            'category_stats': None,

            # Initialize sketches for each numeric column

            for col in self.df.select_dtypes(include=[np.number]).columns:            'sketches': {},class ApproxQueryEngine:import timeimport time

                self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()

                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()            'samples': {}

                self._cache['samples'][col] = ReservoirSampler(1000)

        }    def __init__(self, df: Optional[pd.DataFrame] = None):

                # Populate sketches and samples

                for value in self.df[col]:        self._setup_cache()

                    self._cache['sketches'][f'count_min_{col}'].add(value)

                    self._cache['sketches'][f'hll_{col}'].add(value)        """Initialize the query engine with optional initial DataFrame."""from sketches import HyperLogLog, CountMinSketch, ReservoirSamplerfrom sketches import HyperLogLog, CountMinSketch, ReservoirSampler

                    self._cache['samples'][col].add(value)

    def _setup_cache(self):

    def add_streaming_data(self, new_data: pd.DataFrame) -> None:

        """Add new data in streaming mode."""        """Initialize cache with sketches and samples for key columns."""        self.df = df.copy() if df is not None else pd.DataFrame()

        if not self.streaming_mode:

            raise ValueError("Engine not in streaming mode")        if not self.streaming_mode and len(self.df) > 0:



        # Update main DataFrame            # Initialize category-based statistics        self.streaming_mode = df is Nonefrom collections import defaultdictfrom collections import defaultdict

        self.df = pd.concat([self.df, new_data], ignore_index=True)

        self._cache['total_rows'] += len(new_data)            self._cache['category_stats'] = (



        # Update sketches and samples                self.df.groupby('category', as_index=False)        self._cache = {

        for col in new_data.select_dtypes(include=[np.number]).columns:

            if col not in self._cache['sketches']:                .agg({'price': ['count', 'mean', 'sum']})

                self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()

                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()            )            'total_rows': len(self.df) if df is not None else 0,

                self._cache['samples'][col] = ReservoirSampler(1000)

            self._cache['category_stats'].columns = ['category', 'count', 'avg_price', 'sum_price']

            for value in new_data[col]:

                self._cache['sketches'][f'count_min_{col}'].add(value)            'category_stats': None,

                self._cache['sketches'][f'hll_{col}'].add(value)

                self._cache['samples'][col].add(value)            # Initialize sketches for each numeric column



    def run_query(self, query: str) -> Dict[str, Any]:            for col in self.df.select_dtypes(include=[np.number]).columns:            'sketches': {},class ApproxQueryEngine:class ApproxQueryEngine:

        """Run a query with support for exact and approximate results."""

        try:                self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()

            q = self.parse_query(query)

            start = time.perf_counter()                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()            'samples': {}

            

            if q["is_approx"]:                self._cache['samples'][col] = ReservoirSampler(1000)

                result = self._run_approximate(q)

            else:        }    def __init__(self, df: Optional[pd.DataFrame] = None):    from typing import Dict, Any, List, Union, Optional, Tuple

                result = self._run_exact(q)

                                # Populate sketches and samples

            elapsed = time.perf_counter() - start

            return {**result, "time": elapsed}                for value in self.df[col]:        self._setup_cache()

        except Exception as e:

            return {"error": str(e), "time": 0.001}                    self._cache['sketches'][f'count_min_{col}'].add(value)



    def parse_query(self, query: str) -> Dict[str, Any]:                    self._cache['sketches'][f'hll_{col}'].add(value)        """Initialize the query engine with optional initial DataFrame.import pandas as pd

        """Parse SQL-like query with support for approximation and accuracy settings."""

        pattern = r"(APPROXIMATE\s+)?SELECT\s+(.*?)\s+FROM\s+(\w+)(?:\s+GROUP BY\s+(.*?))?(?:\s+WITH ACCURACY\s+(\d+)%?)?(?:\s+WITHIN\s+(\d+)%\s+CONFIDENCE)?;"                    self._cache['samples'][col].add(value)

        m = re.match(pattern, query.strip(), re.IGNORECASE)

        if not m:    def _setup_cache(self):

            raise ValueError("Invalid query format")

        return {    def add_streaming_data(self, new_data: pd.DataFrame) -> None:

            "is_approx": bool(m.group(1)),

            "select_expr": m.group(2),        """Add new data in streaming mode."""        """Initialize cache with sketches and samples for key columns."""        Supports both batch and streaming modes."""import numpy as np

            "table": m.group(3),

            "group_by": m.group(4),        if not self.streaming_mode:

            "accuracy": int(m.group(5)) if m.group(5) else 95,

            "confidence": int(m.group(6)) if m.group(6) else 95            raise ValueError("Engine not in streaming mode")        if not self.streaming_mode and len(self.df) > 0:

        }



    def _run_exact(self, q: Dict[str, Any]) -> Dict[str, Any]:

        """Run exact query computation."""        # Update main DataFrame            # Initialize category-based statistics        self.df = df.copy() if df is not None else pd.DataFrame()import re

        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:

            return {        self.df = pd.concat([self.df, new_data], ignore_index=True)

                "result": pd.DataFrame([{"count": self._cache['total_rows']}]),

                "approx": False        self._cache['total_rows'] += len(new_data)            self._cache['category_stats'] = (

            }



        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None

        agg_dict = self._parse_select_expr(q["select_expr"])        # Update sketches and samples                self.df.groupby('category', as_index=False)        self.streaming_mode = df is Noneimport time

        

        if group_cols:        for col in new_data.select_dtypes(include=[np.number]).columns:

            result = self.df.groupby(group_cols).agg(**agg_dict).reset_index()

        else:            if col not in self._cache['sketches']:                .agg({'price': ['count', 'mean', 'sum']})

            result = pd.DataFrame([self.df.agg(**agg_dict)])

                            self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()

        return {"result": result, "approx": False}

                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()            )        self._cache = {from sketches import HyperLogLog, CountMinSketch, ReservoirSampler

    def _run_approximate(self, q: Dict[str, Any]) -> Dict[str, Any]:

        """Run approximate query using sketches and sampling."""                self._cache['samples'][col] = ReservoirSampler(1000)

        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:

            # Use HyperLogLog for distinct counts            self._cache['category_stats'].columns = ['category', 'count', 'avg_price', 'sum_price']

            if "DISTINCT" in q["select_expr"].upper():

                hll = self._cache['sketches'].get('hll_id', HyperLogLog())            for value in new_data[col]:

                count = hll.count()

            else:                self._cache['sketches'][f'count_min_{col}'].add(value)            'total_rows': len(self.df) if df is not None else 0,from collections import defaultdict

                count = self._cache['total_rows']

                            self._cache['sketches'][f'hll_{col}'].add(value)

            return {

                "result": pd.DataFrame([{"count": count}]),                self._cache['samples'][col].add(value)            # Initialize sketches for each numeric column

                "approx": True,

                "ci": self._calculate_confidence_interval(q["confidence"], count)

            }

    def run_query(self, query: str) -> Dict[str, Any]:            for col in self.df.select_dtypes(include=[np.number]).columns:            'category_stats': None,

        sample_size = self._get_sample_size(q["accuracy"])

        sample = self._get_stratified_sample(sample_size, q) if q["group_by"] else self.df.sample(n=sample_size)        """Run a query with support for exact and approximate results."""

        

        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None        try:                self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()

        agg_dict = self._parse_select_expr(q["select_expr"])

                    q = self.parse_query(query)

        if group_cols:

            result = sample.groupby(group_cols).agg(**agg_dict).reset_index()            start = time.perf_counter()                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()            'sketches': {},class ApproxQueryEngine:

            scale = len(self.df) / len(sample)

            for col in result.columns:            

                if col not in group_cols and ('count' in col.lower() or 'sum' in col.lower()):

                    result[col] *= scale            if q["is_approx"]:                self._cache['samples'][col] = ReservoirSampler(1000)

        else:

            result = pd.DataFrame([sample.agg(**agg_dict)])                result = self._run_approximate(q)

            

        ci = self._calculate_confidence_interval(q["confidence"], sample_size)            else:            'samples': {}    def __init__(self, df: Optional[pd.DataFrame] = None):

        return {

            "result": result,                 result = self._run_exact(q)

            "approx": True, 

            "ci": ci,                                # Populate sketches and samples

            "sample_size": sample_size,

            "total_size": len(self.df)            elapsed = time.perf_counter() - start

        }

            return {**result, "time": elapsed}                for value in self.df[col]:        }        """Initialize the query engine with optional initial DataFrame.

    def _get_stratified_sample(self, sample_size: int, query: Dict[str, Any]) -> pd.DataFrame:

        """Get a stratified sample based on GROUP BY columns."""        except Exception as e:

        group_cols = [c.strip() for c in query["group_by"].split(",")]

        groups = self.df.groupby(group_cols)            return {"error": str(e), "time": 0.001}                    self._cache['sketches'][f'count_min_{col}'].add(value)

        

        # Calculate per-group sample sizes proportionally

        total_rows = len(self.df)

        samples = []    def parse_query(self, query: str) -> Dict[str, Any]:                    self._cache['sketches'][f'hll_{col}'].add(value)        self._setup_cache()        Supports both batch and streaming modes."""

        

        for name, group in groups:        """Parse SQL-like query with support for approximation and accuracy settings."""

            group_size = len(group)

            group_sample_size = max(1, int(sample_size * (group_size / total_rows)))        pattern = r"(APPROXIMATE\s+)?SELECT\s+(.*?)\s+FROM\s+(\w+)(?:\s+GROUP BY\s+(.*?))?(?:\s+WITH ACCURACY\s+(\d+)%?)?(?:\s+WITHIN\s+(\d+)%\s+CONFIDENCE)?;"                    self._cache['samples'][col].add(value)

            samples.append(group.sample(n=min(group_sample_size, group_size)))

                    m = re.match(pattern, query.strip(), re.IGNORECASE)

        return pd.concat(samples)

        if not m:        self.df = df.copy() if df is not None else pd.DataFrame()

    def _calculate_confidence_interval(self, confidence: int, sample_size: int) -> float:

        """Calculate confidence interval based on sample size and desired confidence level."""            raise ValueError("Invalid query format")

        z_score = {

            90: 1.645,        return {    def add_streaming_data(self, new_data: pd.DataFrame) -> None:

            95: 1.96,

            99: 2.576            "is_approx": bool(m.group(1)),

        }.get(confidence, 1.96)

                    "select_expr": m.group(2),        """Add new data in streaming mode."""    def _setup_cache(self):        self.streaming_mode = df is None

        return z_score * (1.0 / np.sqrt(sample_size))

            "table": m.group(3),

    def _get_sample_size(self, accuracy: int) -> int:

        """Calculate required sample size based on desired accuracy."""            "group_by": m.group(4),        if not self.streaming_mode:

        base_size = len(self.df)

        min_size = 1000            "accuracy": int(m.group(5)) if m.group(5) else 95,

        max_size = min(100000, base_size)

                    "confidence": int(m.group(6)) if m.group(6) else 95            raise ValueError("Engine not in streaming mode")        """Initialize cache with sketches and samples for key columns."""        self._cache = {

        # Using statistical formula for sample size

        z_score = 1.96  # 95% confidence level        }

        margin_of_error = (100 - accuracy) / 100.0

        sample_size = int((z_score**2 * 0.25) / (margin_of_error**2))

        

        return max(min_size, min(max_size, sample_size))    def _run_exact(self, q: Dict[str, Any]) -> Dict[str, Any]:



    def _parse_select_expr(self, expr: str) -> Dict[str, str]:        """Run exact query computation."""        # Update main DataFrame        if not self.streaming_mode and len(self.df) > 0:            'total_rows': len(self.df) if df is not None else 0,

        """Parse SELECT expressions into aggregation dictionary."""

        agg_dict = {}        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:

        for item in expr.split(','):

            item = item.strip().upper()            return {        self.df = pd.concat([self.df, new_data], ignore_index=True)

            if 'COUNT(*)' in item:

                agg_dict['size'] = 'count'                "result": pd.DataFrame([{"count": self._cache['total_rows']}]),

            elif 'COUNT(DISTINCT' in item:

                m = re.search(r'COUNT\(DISTINCT\s+(\w+)\)', item, re.IGNORECASE)                "approx": False        self._cache['total_rows'] += len(new_data)            # Initialize category-based statistics            'category_stats': None,

                if m:

                    agg_dict[m.group(1)] = 'nunique'            }

            elif 'COUNT(' in item:

                m = re.search(r'COUNT\((\w+)\)', item, re.IGNORECASE)

                if m:

                    agg_dict[m.group(1)] = 'count'        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None

            elif 'AVG(' in item:

                m = re.search(r'AVG\((\w+)\)', item, re.IGNORECASE)        agg_dict = self._parse_select_expr(q["select_expr"])        # Update sketches and samples            self._cache['category_stats'] = (            'sketches': {},

                if m:

                    agg_dict[m.group(1)] = 'mean'        

            elif 'SUM(' in item:

                m = re.search(r'SUM\((\w+)\)', item, re.IGNORECASE)        if group_cols:        for col in new_data.select_dtypes(include=[np.number]).columns:

                if m:

                    agg_dict[m.group(1)] = 'sum'            result = self.df.groupby(group_cols, as_index=False).agg(agg_dict)

            elif item != '*':

                agg_dict[item] = 'first'        else:            if col not in self._cache['sketches']:                self.df.groupby('category', as_index=False)            'samples': {}

        return agg_dict

            result = pd.DataFrame([self.df.agg(agg_dict)])

    def print_result(self, res: Dict[str, Any]):

        print("Approximate result:" if res.get("approx") else "Exact result:")                            self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()

        print(res["result"])

        print(f"Time taken: {res['time']:.4f} seconds")        return {"result": result, "approx": False}

        if res.get("approx"):

            print(f"Approximate error bound: ±{res['ci']*100:.2f}%")                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()                .agg({'price': ['count', 'mean', 'sum']})        }



# Example usage:    def _run_approximate(self, q: Dict[str, Any]) -> Dict[str, Any]:

if __name__ == "__main__":

    # Load or generate data        """Run approximate query using sketches and sampling."""                self._cache['samples'][col] = ReservoirSampler(1000)

    from synthetic_data import generate_sales_data

    df = generate_sales_data(1_000_000)        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:

    engine = ApproxQueryEngine(df)

    # Exact query            # Use HyperLogLog for distinct counts            )        self._setup_cache()

    q1 = "SELECT category, COUNT(*), AVG(price) FROM sales GROUP BY category;"

    res1 = engine.run_query(q1)            if "DISTINCT" in q["select_expr"].upper():

    engine.print_result(res1)

    # Approximate query                hll = self._cache['sketches'].get('hll_id', HyperLogLog())            for value in new_data[col]:

    q2 = "APPROXIMATE SELECT category, COUNT(*), AVG(price) FROM sales GROUP BY category WITH ACCURACY 95%;"

    res2 = engine.run_query(q2)                count = hll.count()

    engine.print_result(res2)
            else:                self._cache['sketches'][f'count_min_{col}'].add(value)            self._cache['category_stats'].columns = ['category', 'count', 'avg_price', 'sum_price']

                count = self._cache['total_rows']

                            self._cache['sketches'][f'hll_{col}'].add(value)

            return {

                "result": pd.DataFrame([{"count": count}]),                self._cache['samples'][col].add(value)    def _setup_cache(self):

                "approx": True,

                "ci": self._calculate_confidence_interval(q["confidence"], count)

            }

    def run_query(self, query: str) -> Dict[str, Any]:            # Initialize sketches for each numeric column        """Initialize cache with sketches and samples for key columns."""

        sample_size = self._get_sample_size(q["accuracy"])

        sample = self._get_stratified_sample(sample_size, q) if q["group_by"] else self.df.sample(n=sample_size)        """Run a query with support for exact and approximate results."""

        

        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None        try:            for col in self.df.select_dtypes(include=[np.number]).columns:        if not self.streaming_mode and len(self.df) > 0:

        agg_dict = self._parse_select_expr(q["select_expr"])

                    q = self.parse_query(query)

        if group_cols:

            result = sample.groupby(group_cols, as_index=False).agg(agg_dict)            start = time.perf_counter()                self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()            # Initialize category-based statistics

            scale = len(self.df) / len(sample)

            for col in result.columns:            

                if col not in group_cols and ('count' in col.lower() or 'sum' in col.lower()):

                    result[col] *= scale            if q["is_approx"]:                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()            self._cache['category_stats'] = (

        else:

            result = pd.DataFrame([sample.agg(agg_dict)])                result = self._run_approximate(q)

            

        ci = self._calculate_confidence_interval(q["confidence"], sample_size)            else:                self._cache['samples'][col] = ReservoirSampler(1000)                self.df.groupby('category', as_index=False)

        return {

            "result": result,                 result = self._run_exact(q)

            "approx": True, 

            "ci": ci,                                .agg({'price': ['count', 'mean', 'sum']})

            "sample_size": sample_size,

            "total_size": len(self.df)            elapsed = time.perf_counter() - start

        }

            return {**result, "time": elapsed}                # Populate sketches and samples            )

    def _get_stratified_sample(self, sample_size: int, query: Dict[str, Any]) -> pd.DataFrame:

        """Get a stratified sample based on GROUP BY columns."""        except Exception as e:

        group_cols = [c.strip() for c in query["group_by"].split(",")]

        groups = self.df.groupby(group_cols)            return {"error": str(e), "time": 0.001}                for value in self.df[col]:            self._cache['category_stats'].columns = ['category', 'count', 'avg_price', 'sum_price']

        

        # Calculate per-group sample sizes proportionally

        total_rows = len(self.df)

        samples = []    def parse_query(self, query: str) -> Dict[str, Any]:                    self._cache['sketches'][f'count_min_{col}'].add(value)

        

        for name, group in groups:        """Parse SQL-like query with support for approximation and accuracy settings."""

            group_size = len(group)

            group_sample_size = max(1, int(sample_size * (group_size / total_rows)))        pattern = r"(APPROXIMATE\s+)?SELECT\s+(.*?)\s+FROM\s+(\w+)(?:\s+GROUP BY\s+(.*?))?(?:\s+WITH ACCURACY\s+(\d+)%?)?(?:\s+WITHIN\s+(\d+)%\s+CONFIDENCE)?;"                    self._cache['sketches'][f'hll_{col}'].add(value)            # Initialize sketches for each numeric column

            samples.append(group.sample(n=min(group_sample_size, group_size)))

                    m = re.match(pattern, query.strip(), re.IGNORECASE)

        return pd.concat(samples)

        if not m:                    self._cache['samples'][col].add(value)            for col in self.df.select_dtypes(include=[np.number]).columns:

    def _calculate_confidence_interval(self, confidence: int, sample_size: int) -> float:

        """Calculate confidence interval based on sample size and desired confidence level."""            raise ValueError("Invalid query format")

        z_score = {

            90: 1.645,        return {                self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()

            95: 1.96,

            99: 2.576            "is_approx": bool(m.group(1)),

        }.get(confidence, 1.96)

                    "select_expr": m.group(2),    def add_streaming_data(self, new_data: pd.DataFrame) -> None:                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()

        return z_score * (1.0 / np.sqrt(sample_size))

            "table": m.group(3),

    def _get_sample_size(self, accuracy: int) -> int:

        """Calculate required sample size based on desired accuracy."""            "group_by": m.group(4),        """Add new data in streaming mode."""                self._cache['samples'][col] = ReservoirSampler(1000)

        base_size = len(self.df)

        min_size = 1000            "accuracy": int(m.group(5)) if m.group(5) else 95,

        max_size = min(100000, base_size)

                    "confidence": int(m.group(6)) if m.group(6) else 95        if not self.streaming_mode:

        # Using statistical formula for sample size

        z_score = 1.96  # 95% confidence level        }

        margin_of_error = (100 - accuracy) / 100.0

        sample_size = int((z_score**2 * 0.25) / (margin_of_error**2))            raise ValueError("Engine not in streaming mode")                # Populate sketches and samples

        

        return max(min_size, min(max_size, sample_size))    def _run_exact(self, q: Dict[str, Any]) -> Dict[str, Any]:



    def _parse_select_expr(self, expr: str) -> Dict[str, List[str]]:        """Run exact query computation."""                for value in self.df[col]:

        """Parse SELECT expressions into aggregation dictionary."""

        agg_dict = defaultdict(list)        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:

        

        for item in expr.split(','):            return {        # Update main DataFrame                    self._cache['sketches'][f'count_min_{col}'].add(value)

            item = item.strip().upper()

            if 'COUNT(*)' in item:                "result": pd.DataFrame([{"count": self._cache['total_rows']}]),

                agg_dict['*'].append('count')

            elif 'COUNT(DISTINCT' in item:                "approx": False        self.df = pd.concat([self.df, new_data], ignore_index=True)                    self._cache['sketches'][f'hll_{col}'].add(value)

                m = re.search(r'COUNT\(DISTINCT\s+(\w+)\)', item, re.IGNORECASE)

                if m:            }

                    agg_dict[m.group(1)].append('nunique')

            elif 'COUNT(' in item:        self._cache['total_rows'] += len(new_data)                    self._cache['samples'][col].add(value)

                m = re.search(r'COUNT\((\w+)\)', item, re.IGNORECASE)

                if m:        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None

                    agg_dict[m.group(1)].append('count')

            elif 'AVG(' in item:        agg_dict = self._parse_select_expr(q["select_expr"])

                m = re.search(r'AVG\((\w+)\)', item, re.IGNORECASE)

                if m:        

                    agg_dict[m.group(1)].append('mean')

            elif 'SUM(' in item:        if group_cols:        # Update sketches and samples    def run_query(self, query: str) -> Dict[str, Any]:

                m = re.search(r'SUM\((\w+)\)', item, re.IGNORECASE)

                if m:            result = self.df.groupby(group_cols, as_index=False).agg(agg_dict)

                    agg_dict[m.group(1)].append('sum')

            elif item != '*':        else:        for col in new_data.select_dtypes(include=[np.number]).columns:        """Run a query with support for exact and approximate results."""

                agg_dict[item].append('first')

                    result = pd.DataFrame([self.df.agg(agg_dict)])

        return dict(agg_dict)
                        if col not in self._cache['sketches']:        try:

        return {"result": result, "approx": False}

                self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()            q = self.parse_query(query)

    def _run_approximate(self, q: Dict[str, Any]) -> Dict[str, Any]:

        """Run approximate query using sketches and sampling."""                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()            start = time.perf_counter()

        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:

            # Use HyperLogLog for distinct counts                self._cache['samples'][col] = ReservoirSampler(1000)            

            if "DISTINCT" in q["select_expr"].upper():

                hll = self._cache['sketches'].get('hll_id', HyperLogLog())            if q["is_approx"]:

                count = hll.count()

            else:            for value in new_data[col]:                result = self._run_approximate(q)

                count = self._cache['total_rows']

                            self._cache['sketches'][f'count_min_{col}'].add(value)            else:

            return {

                "result": pd.DataFrame([{"count": count}]),                self._cache['sketches'][f'hll_{col}'].add(value)                result = self._run_exact(q)

                "approx": True,

                "ci": self._calculate_confidence_interval(q["confidence"], count)                self._cache['samples'][col].add(value)                

            }

            elapsed = time.perf_counter() - start

        sample_size = self._get_sample_size(q["accuracy"])

        sample = self._get_stratified_sample(sample_size, q) if q["group_by"] else self.df.sample(n=sample_size)    def run_query(self, query: str) -> Dict[str, Any]:            return {**result, "time": elapsed}

        

        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None        """Run a query with support for exact and approximate results."""        except Exception as e:

        agg_dict = self._parse_select_expr(q["select_expr"])

                try:            return {"error": str(e), "time": 0.001}

        if group_cols:

            result = sample.groupby(group_cols, as_index=False).agg(agg_dict)            q = self.parse_query(query)

            scale = len(self.df) / len(sample)

            for col in result.columns:            start = time.perf_counter()    def add_streaming_data(self, new_data: pd.DataFrame) -> None:

                if col not in group_cols and ('count' in col.lower() or 'sum' in col.lower()):

                    result[col] *= scale                    """Add new data in streaming mode."""

        else:

            result = pd.DataFrame([sample.agg(agg_dict)])            if q["is_approx"]:        if not self.streaming_mode:

            

        ci = self._calculate_confidence_interval(q["confidence"], sample_size)                result = self._run_approximate(q)            raise ValueError("Engine not in streaming mode")

        return {

            "result": result,             else:

            "approx": True, 

            "ci": ci,                result = self._run_exact(q)        # Update main DataFrame

            "sample_size": sample_size,

            "total_size": len(self.df)                        self.df = pd.concat([self.df, new_data], ignore_index=True)

        }

            elapsed = time.perf_counter() - start        self._cache['total_rows'] += len(new_data)

    def _get_stratified_sample(self, sample_size: int, query: Dict[str, Any]) -> pd.DataFrame:

        """Get a stratified sample based on GROUP BY columns."""            return {**result, "time": elapsed}

        group_cols = [c.strip() for c in query["group_by"].split(",")]

        groups = self.df.groupby(group_cols)        except Exception as e:        # Update sketches and samples

        

        # Calculate per-group sample sizes proportionally            return {"error": str(e), "time": 0.001}        for col in new_data.select_dtypes(include=[np.number]).columns:

        total_rows = len(self.df)

        samples = []            if col not in self._cache['sketches']:

        

        for name, group in groups:    def parse_query(self, query: str) -> Dict[str, Any]:                self._cache['sketches'][f'count_min_{col}'] = CountMinSketch()

            group_size = len(group)

            group_sample_size = max(1, int(sample_size * (group_size / total_rows)))        """Parse SQL-like query with support for approximation and accuracy settings."""                self._cache['sketches'][f'hll_{col}'] = HyperLogLog()

            samples.append(group.sample(n=min(group_sample_size, group_size)))

                    pattern = r"(APPROXIMATE\s+)?SELECT\s+(.*?)\s+FROM\s+(\w+)(?:\s+GROUP BY\s+(.*?))?(?:\s+WITH ACCURACY\s+(\d+)%?)?(?:\s+WITHIN\s+(\d+)%\s+CONFIDENCE)?;"                self._cache['samples'][col] = ReservoirSampler(1000)

        return pd.concat(samples)

        m = re.match(pattern, query.strip(), re.IGNORECASE)

    def _calculate_confidence_interval(self, confidence: int, sample_size: int) -> float:

        """Calculate confidence interval based on sample size and desired confidence level."""        if not m:            for value in new_data[col]:

        z_score = {

            90: 1.645,            raise ValueError("Invalid query format")                self._cache['sketches'][f'count_min_{col}'].add(value)

            95: 1.96,

            99: 2.576        return {                self._cache['sketches'][f'hll_{col}'].add(value)

        }.get(confidence, 1.96)

                    "is_approx": bool(m.group(1)),                self._cache['samples'][col].add(value)

        return z_score * (1.0 / np.sqrt(sample_size))

            "select_expr": m.group(2),

    def _get_sample_size(self, accuracy: int) -> int:

        """Calculate required sample size based on desired accuracy."""            "table": m.group(3),    def parse_query(self, query: str) -> Dict[str, Any]:

        base_size = len(self.df)

        min_size = 1000            "group_by": m.group(4),        """Parse SQL-like query with support for approximation and accuracy settings."""

        max_size = min(100000, base_size)

                    "accuracy": int(m.group(5)) if m.group(5) else 95,        pattern = r"(APPROXIMATE\s+)?SELECT\s+(.*?)\s+FROM\s+(\w+)(?:\s+GROUP BY\s+(.*?))?(?:\s+WITH ACCURACY\s+(\d+)%?)?(?:\s+WITHIN\s+(\d+)%\s+CONFIDENCE)?;"

        # Using statistical formula for sample size

        z_score = 1.96  # 95% confidence level            "confidence": int(m.group(6)) if m.group(6) else 95        m = re.match(pattern, query.strip(), re.IGNORECASE)

        margin_of_error = (100 - accuracy) / 100.0

        sample_size = int((z_score**2 * 0.25) / (margin_of_error**2))        }        if not m:

        

        return max(min_size, min(max_size, sample_size))            raise ValueError("Invalid query format")



    def _parse_select_expr(self, expr: str) -> Dict[str, List[str]]:    def _run_exact(self, q: Dict[str, Any]) -> Dict[str, Any]:        return {

        """Parse SELECT expressions into aggregation dictionary."""

        agg_dict = defaultdict(list)        """Run exact query computation."""            "is_approx": bool(m.group(1)),

        

        for item in expr.split(','):        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:            "select_expr": m.group(2),

            item = item.strip().upper()

            if 'COUNT(*)' in item:            return {            "table": m.group(3),

                agg_dict['*'].append('count')

            elif 'COUNT(DISTINCT' in item:                "result": pd.DataFrame([{"count": self._cache['total_rows']}]),            "group_by": m.group(4),

                m = re.search(r'COUNT\(DISTINCT\s+(\w+)\)', item, re.IGNORECASE)

                if m:                "approx": False            "accuracy": int(m.group(5)) if m.group(5) else 95,

                    agg_dict[m.group(1)].append('nunique')

            elif 'COUNT(' in item:            }            "confidence": int(m.group(6)) if m.group(6) else 95

                m = re.search(r'COUNT\((\w+)\)', item, re.IGNORECASE)

                if m:        }

                    agg_dict[m.group(1)].append('count')

            elif 'AVG(' in item:        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None

                m = re.search(r'AVG\((\w+)\)', item, re.IGNORECASE)

                if m:        agg_dict = self._parse_select_expr(q["select_expr"])    def _run_exact(self, q: Dict[str, Any]) -> Dict[str, Any]:

                    agg_dict[m.group(1)].append('mean')

            elif 'SUM(' in item:                """Run exact query computation."""

                m = re.search(r'SUM\((\w+)\)', item, re.IGNORECASE)

                if m:        if group_cols:        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:

                    agg_dict[m.group(1)].append('sum')

            elif item != '*':            result = self.df.groupby(group_cols, as_index=False).agg(agg_dict)            return {

                agg_dict[item].append('first')

                else:                "result": pd.DataFrame([{"count": self._cache['total_rows']}]),

        return dict(agg_dict)
            result = pd.DataFrame([self.df.agg(agg_dict)])                "approx": False

                        }

        return {"result": result, "approx": False}

        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None

    def _run_approximate(self, q: Dict[str, Any]) -> Dict[str, Any]:        agg_dict = self._parse_select_expr(q["select_expr"])

        """Run approximate query using sketches and sampling."""        

        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:        if group_cols:

            # Use HyperLogLog for distinct counts            result = self.df.groupby(group_cols, as_index=False).agg(agg_dict)

            if "DISTINCT" in q["select_expr"].upper():        else:

                hll = self._cache['sketches'].get('hll_id', HyperLogLog())            result = pd.DataFrame([self.df.agg(agg_dict)])

                count = hll.count()            

            else:        return {"result": result, "approx": False}

                count = self._cache['total_rows']

                def _run_approximate(self, q: Dict[str, Any]) -> Dict[str, Any]:

            return {        """Run approximate query using sketches and sampling."""

                "result": pd.DataFrame([{"count": count}]),        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:

                "approx": True,            # Use HyperLogLog for distinct counts

                "ci": self._calculate_confidence_interval(q["confidence"], count)            if "DISTINCT" in q["select_expr"].upper():

            }                hll = self._cache['sketches'].get('hll_id', HyperLogLog())

                count = hll.count()

        sample_size = self._get_sample_size(q["accuracy"])            else:

        sample = self._get_stratified_sample(sample_size, q) if q["group_by"] else self.df.sample(n=sample_size)                count = self._cache['total_rows']

                    

        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None            return {

        agg_dict = self._parse_select_expr(q["select_expr"])                "result": pd.DataFrame([{"count": count}]),

                        "approx": True,

        if group_cols:                "ci": self._calculate_confidence_interval(q["confidence"], count)

            result = sample.groupby(group_cols, as_index=False).agg(agg_dict)            }

            scale = len(self.df) / len(sample)

            for col in result.columns:        sample_size = self._get_sample_size(q["accuracy"])

                if col not in group_cols and ('count' in col.lower() or 'sum' in col.lower()):        sample = self._get_stratified_sample(sample_size, q) if q["group_by"] else self.df.sample(n=sample_size)

                    result[col] *= scale        

        else:        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None

            result = pd.DataFrame([sample.agg(agg_dict)])        agg_dict = self._parse_select_expr(q["select_expr"])

                    

        ci = self._calculate_confidence_interval(q["confidence"], sample_size)        if group_cols:

        return {            result = sample.groupby(group_cols, as_index=False).agg(agg_dict)

            "result": result,             scale = len(self.df) / len(sample)

            "approx": True,             for col in result.columns:

            "ci": ci,                if col not in group_cols and ('count' in col.lower() or 'sum' in col.lower()):

            "sample_size": sample_size,                    result[col] *= scale

            "total_size": len(self.df)        else:

        }            result = pd.DataFrame([sample.agg(agg_dict)])

            

    def _get_stratified_sample(self, sample_size: int, query: Dict[str, Any]) -> pd.DataFrame:        ci = self._calculate_confidence_interval(q["confidence"], sample_size)

        """Get a stratified sample based on GROUP BY columns."""        return {

        group_cols = [c.strip() for c in query["group_by"].split(",")]            "result": result, 

        groups = self.df.groupby(group_cols)            "approx": True, 

                    "ci": ci,

        # Calculate per-group sample sizes proportionally            "sample_size": sample_size,

        total_rows = len(self.df)            "total_size": len(self.df)

        samples = []        }

        

        for name, group in groups:    def _get_stratified_sample(self, sample_size: int, query: Dict[str, Any]) -> pd.DataFrame:

            group_size = len(group)        """Get a stratified sample based on GROUP BY columns."""

            group_sample_size = max(1, int(sample_size * (group_size / total_rows)))        group_cols = [c.strip() for c in query["group_by"].split(",")]

            samples.append(group.sample(n=min(group_sample_size, group_size)))        groups = self.df.groupby(group_cols)

                    

        return pd.concat(samples)        # Calculate per-group sample sizes proportionally

        total_rows = len(self.df)

    def _calculate_confidence_interval(self, confidence: int, sample_size: int) -> float:        samples = []

        """Calculate confidence interval based on sample size and desired confidence level."""        

        z_score = {        for name, group in groups:

            90: 1.645,            group_size = len(group)

            95: 1.96,            group_sample_size = max(1, int(sample_size * (group_size / total_rows)))

            99: 2.576            samples.append(group.sample(n=min(group_sample_size, group_size)))

        }.get(confidence, 1.96)            

                return pd.concat(samples)

        return z_score * (1.0 / np.sqrt(sample_size))

    def _calculate_confidence_interval(self, confidence: int, sample_size: int) -> float:

    def _get_sample_size(self, accuracy: int) -> int:        """Calculate confidence interval based on sample size and desired confidence level."""

        """Calculate required sample size based on desired accuracy."""        z_score = {

        base_size = len(self.df)            90: 1.645,

        min_size = 1000            95: 1.96,

        max_size = min(100000, base_size)            99: 2.576

                }.get(confidence, 1.96)

        # Using statistical formula for sample size        

        z_score = 1.96  # 95% confidence level        return z_score * (1.0 / np.sqrt(sample_size))

        margin_of_error = (100 - accuracy) / 100.0

        sample_size = int((z_score**2 * 0.25) / (margin_of_error**2))    def _get_sample_size(self, accuracy: int) -> int:

                """Calculate required sample size based on desired accuracy."""

        return max(min_size, min(max_size, sample_size))        base_size = len(self.df)

        min_size = 1000

    def _parse_select_expr(self, expr: str) -> Dict[str, List[str]]:        max_size = min(100000, base_size)

        """Parse SELECT expressions into aggregation dictionary."""        

        agg_dict = defaultdict(list)        # Using statistical formula for sample size

                z_score = 1.96  # 95% confidence level

        for item in expr.split(','):        margin_of_error = (100 - accuracy) / 100.0

            item = item.strip().upper()        sample_size = int((z_score**2 * 0.25) / (margin_of_error**2))

            if 'COUNT(*)' in item:        

                agg_dict['*'].append('count')        return max(min_size, min(max_size, sample_size))

            elif 'COUNT(DISTINCT' in item:

                m = re.search(r'COUNT\(DISTINCT\s+(\w+)\)', item, re.IGNORECASE)    def _parse_select_expr(self, expr: str) -> Dict[str, List[str]]:

                if m:        """Parse SELECT expressions into aggregation dictionary."""

                    agg_dict[m.group(1)].append('nunique')        agg_dict = defaultdict(list)

            elif 'COUNT(' in item:        

                m = re.search(r'COUNT\((\w+)\)', item, re.IGNORECASE)        for item in expr.split(','):

                if m:            item = item.strip().upper()

                    agg_dict[m.group(1)].append('count')            if 'COUNT(*)' in item:

            elif 'AVG(' in item:                agg_dict['*'].append('count')

                m = re.search(r'AVG\((\w+)\)', item, re.IGNORECASE)            elif 'COUNT(DISTINCT' in item:

                if m:                col = re.search(r'COUNT\(DISTINCT\s+(\w+)\)', item, re.IGNORECASE)

                    agg_dict[m.group(1)].append('mean')                if col:

            elif 'SUM(' in item:                    agg_dict[col.group(1)].append('nunique')

                m = re.search(r'SUM\((\w+)\)', item, re.IGNORECASE)            elif 'COUNT(' in item:

                if m:                col = re.search(r'COUNT\((\w+)\)', item, re.IGNORECASE)

                    agg_dict[m.group(1)].append('sum')                if col:

            elif item != '*':                    agg_dict[col.group(1)].append('count')

                agg_dict[item].append('first')            elif 'AVG(' in item:

                        col = re.search(r'AVG\((\w+)\)', item, re.IGNORECASE)

        return dict(agg_dict)                if col:
                    agg_dict[col.group(1)].append('mean')
            elif 'SUM(' in item:
                col = re.search(r'SUM\((\w+)\)', item, re.IGNORECASE)
                if col:
                    agg_dict[col.group(1)].append('sum')
            elif item != '*':
                agg_dict[item].append('first')
        
        return dict(agg_dict)

    def _parse_select_expr(self, expr: str) -> Dict[str, str]:
        agg_dict = {}
        items = [x.strip() for x in expr.split(",")]
        for item in items:
            if item.upper().startswith("COUNT(*)"):
                agg_dict["count"] = "count"
            elif item.upper().startswith("SUM("):
                col = item[4:-1].strip()
                agg_dict[f"sum_{col}"] = "sum"
            elif item.upper().startswith("AVG("):
                col = item[4:-1].strip()
                agg_dict[f"avg_{col}"] = "mean"
            else:
                agg_dict[item] = "first"
        return agg_dict
                col = item[4:-1].strip()
                agg_dict[f"sum_{col}"] = "sum"
            elif item.upper().startswith("AVG("):
                col = item[4:-1].strip()
                agg_dict[f"avg_{col}"] = "mean"
            else:
                agg_dict[item] = "first"
        return agg_dict
    
    def print_result(self, res: Dict[str, Any]):
        print("Approximate result:" if res.get("approx") else "Exact result:")
        print(res["result"])
        print(f"Time taken: {res['time']:.4f} seconds")
        if res.get("approx"):
            print(f"Approximate error bound: ±{res['ci']*100:.2f}%")

# Example usage:
if __name__ == "__main__":
    # Load or generate data
    from synthetic_data import generate_sales_data
    df = generate_sales_data(1_000_000)
    engine = ApproxQueryEngine(df)
    # Exact query
    q1 = "SELECT category, COUNT(*), AVG(price) FROM sales GROUP BY category;"
    res1 = engine.run_query(q1)
    engine.print_result(res1)
    # Approximate query
    q2 = "APPROXIMATE SELECT category, COUNT(*), AVG(price) FROM sales GROUP BY category WITH ACCURACY 95%;"
    res2 = engine.run_query(q2)
    engine.print_result(res2)
        print(res["result"])
        print(f"Time taken: {res['time']:.4f} seconds")
        if res.get("approx"):
            print(f"Approximate error bound: ±{res['ci']*100:.2f}%")

# Example usage:
if __name__ == "__main__":
    # Load or generate data
    from synthetic_data import generate_sales_data
    df = generate_sales_data(1_000_000)
    engine = ApproxQueryEngine(df)
    # Exact query
    q1 = "SELECT category, COUNT(*), AVG(price) FROM sales GROUP BY category;"
    res1 = engine.run_query(q1)
    engine.print_result(res1)
    # Approximate query
    q2 = "APPROXIMATE SELECT category, COUNT(*), AVG(price) FROM sales GROUP BY category WITH ACCURACY 95%;"
    res2 = engine.run_query(q2)
    engine.print_result(res2)
            return pd.DataFrame({
                "category": self._precomputed_stats['category_counts'].index,
                "count": self._precomputed_stats['category_counts'].values,
                "avg_price": self._precomputed_stats['category_avgs'].values
            })

    def _process_chunk(self, chunk: pd.DataFrame, q: Dict[str, Any]):
        """Process a chunk of data for approximate query"""
        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None
        agg_dict = self._parse_select_expr(q["select_expr"])
        
        if group_cols:
            return (chunk.groupby(group_cols)
                     .agg(agg_dict)
                     .rename_axis(index=group_cols))
        else:
            return chunk.agg(agg_dict).to_frame().T

    def _merge_results(self, results: list, q: Dict[str, Any]):
        """Merge results from parallel processing"""
        group_cols = [c.strip() for c in q["group_by"].split(",")] if q["group_by"] else None
        if group_cols:
            # Concatenate and group by to merge results
            merged = pd.concat(results)
            return (merged.groupby(group_cols)
                    .sum()
                    .rename_axis(index=group_cols)
                    .reset_index())
        else:
            # Simple case, just sum the results
            return sum(results)

    def _calculate_ci(self, result: pd.DataFrame, accuracy: int) -> float:
        """Calculate confidence interval for the result"""
        sample_size = len(result)
        ci = 1.96 * np.sqrt((result['count'] * (1 - result['count'] / self._total_rows)) / sample_size)
        return ci.mean() if not ci.empty else 0.0

    def _get_query_type(self, q: Dict[str, Any]) -> str:
        if "COUNT(*)" in q["select_expr"] and not q["group_by"]:
            return "simple_count"
        elif q["group_by"]:
            return "group_by"
        return "complex"

    def _get_optimized_sample(self, sample_size: int, query_type: str) -> pd.DataFrame:
        if query_type == "group_by" and self._category_counts is not None:
            # Stratified sampling
            return self.df.groupby('category').apply(
                lambda x: x.sample(n=max(1, int(sample_size * len(x)/len(self.df))))
            ).reset_index(drop=True)
        else:
            # Simple random sampling
            return self.df.sample(n=sample_size)

    def _parse_select_expr(self, expr: str) -> Dict[str, Any]:
        agg_dict = {}
        items = [x.strip() for x in expr.split(",")]
        for item in items:
            if item.upper().startswith("COUNT(*)"):
                # Use a unique name for count column
                agg_dict["count_total"] = ("id", "count") if "id" in self.df.columns else (self.df.columns[0], "count")
            elif item.upper().startswith("SUM("):
                col = item[4:-1].strip()
                agg_dict[f"sum_{col}"] = (col, "sum")
            elif item.upper().startswith("AVG("):
                col = item[4:-1].strip()
                agg_dict[f"avg_{col}"] = (col, "mean")
            else:
                # Use original column name without duplication
                if item not in agg_dict:
                    agg_dict[item] = (item, "first")
        return {v[0]: v[1] for k, v in agg_dict.items()}
    
    def print_result(self, res: Dict[str, Any]):
        print("Approximate result:" if res.get("approx") else "Exact result:")
        print(res["result"])
        print(f"Time taken: {res['time']:.4f} seconds")
        if res.get("approx"):
            print(f"Approximate error bound: ±{res['ci']*100:.2f}%")

# Example usage:
if __name__ == "__main__":
    # Load or generate data
    from synthetic_data import generate_sales_data
    df = generate_sales_data(1_000_000)
    engine = ApproxQueryEngine(df)
    # Exact query
    q1 = "SELECT category, COUNT(*), AVG(price) FROM sales GROUP BY category;"
    res1 = engine.run_query(q1)
    engine.print_result(res1)
    # Approximate query
    q2 = "APPROXIMATE SELECT category, COUNT(*), AVG(price) FROM sales GROUP BY category WITH ACCURACY 95%;"
    res2 = engine.run_query(q2)
    engine.print_result(res2)
